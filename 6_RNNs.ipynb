{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "HX5FvAGMcuM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 6. RNN\n",
        "### Importy"
      ]
    },
    {
      "metadata": {
        "id": "-4lscznLcuM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# imports \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from typing import Tuple, Optional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HspHYfI-cuND",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ładowanie danych\n",
        "Na tych zajęciach będziemy traktować cyfry z MNISTa jako dane sekwencyjne, gdzie w danym kroku czasowym $T$ obserwujemy $T$-ty wiersz pikseli z cyfry."
      ]
    },
    {
      "metadata": {
        "id": "lP6JpqyYcuNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transforms = Compose([ToTensor(), Lambda(lambda x: x.reshape(28, 28))])\n",
        "\n",
        "train_data = MNIST(root='.', train=True, transform=transforms, download=True)\n",
        "test_data = MNIST(root='.', train=False, transform=transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQJo0Eg6cuNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zadanie 1.\n",
        "\n",
        "Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n",
        "![rnn](utils/rnn.png)\n",
        "\n",
        "* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n",
        "* Wyjście z sieci możemy mieć dowolny rozmiar, potrzebna jest również warstwa przekształcająca stan ukryty na wyjście.\n",
        "* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n"
      ]
    },
    {
      "metadata": {
        "id": "RXXuRwxVcuNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 hidden_size: int, \n",
        "                 output_size: int):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.input_to_hidden = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_to_hidden = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_to_output = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "    \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "\n",
        "        hidden = torch.tanh(self.input_to_hidden(input) + self.hidden_to_hidden(hidden))\n",
        "        hidden.retain_grad()\n",
        "        output = self.hidden_to_output(hidden)\n",
        "        \n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTuHB5whcuNM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "CimBJ-HZcuNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "f08e3961-725a-43ea-8b01-bc29a3f47c0e"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# build data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# initialize network and optimizer\n",
        "rnn = RNN(28, 64, 10)\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)   \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epochs = 1\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epochs):    \n",
        "    grads = []\n",
        "    for i, (x, y) in enumerate(train_loader):  \n",
        "        optimizer.zero_grad()\n",
        "        # get initial hidden state\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        \n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the 2nd, time dimensiotn\n",
        "        \n",
        "        hiddens = []\n",
        "        seq_len = x.shape[1]\n",
        "        for t in range(seq_len):\n",
        "            output, hidden = rnn(x[:,t,:], hidden)\n",
        "            hiddens.append(hidden)\n",
        "\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        grads.append([np.linalg.norm(h.grad.numpy()) for h in hiddens])\n",
        "        \n",
        "        if i % 100 == 1:\n",
        "            print(f\"Epoch: {epoch} Iter: {i}/{len(train_loader)} Loss: {loss}\")\n",
        "\n",
        "np_grads = np.array([np.array(xi) for xi in grads])\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    \n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        for t in range(seq_len):\n",
        "            output, hidden = rnn(x[:, t, :], hidden)\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += int(sum(pred == y))\n",
        "    \n",
        "    accuracy = correct / (batch_size * len(test_loader))\n",
        "\n",
        "    print(f\"Final Accuracy: {accuracy}\")\n",
        "    assert accuracy > 0.4, \"Subject to random seed you should get over 0.4 accuracy, try changing the seed!\""
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Iter: 1/1200 Loss: 2.325571060180664\n",
            "Epoch: 0 Iter: 101/1200 Loss: 1.614290714263916\n",
            "Epoch: 0 Iter: 201/1200 Loss: 1.0623154640197754\n",
            "Epoch: 0 Iter: 301/1200 Loss: 0.9315030574798584\n",
            "Epoch: 0 Iter: 401/1200 Loss: 0.8028134703636169\n",
            "Epoch: 0 Iter: 501/1200 Loss: 0.9220730662345886\n",
            "Epoch: 0 Iter: 601/1200 Loss: 0.9503166079521179\n",
            "Epoch: 0 Iter: 701/1200 Loss: 0.7411617040634155\n",
            "Epoch: 0 Iter: 801/1200 Loss: 1.2632415294647217\n",
            "Epoch: 0 Iter: 901/1200 Loss: 1.1591308116912842\n",
            "Epoch: 0 Iter: 1001/1200 Loss: 1.3274043798446655\n",
            "Epoch: 0 Iter: 1101/1200 Loss: 1.457504153251648\n",
            "Final Accuracy: 0.4464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "su14TPs7cuNQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zadanie 2\n",
        "Dopisz kod do powyższej pętli, który zbiera gradienty po kolejnych stanach ukrytych dla przykładu. Pokaż (za pomocą wykresu numpy) efekt zanikającego gradientu rysując średnie normy gradientu dla każdego stanu ukrytego po każdym kroku. HINT: dla MNISTa mamy 28 kroków. \n",
        "\n",
        "**Ważne:** Ponieważ normalnie w torchu czyścimy wszystkie gradienty po każdej iteracji aby je zachować w dla niektórych wag przydatna będzie metoda [`retain_grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.retain_grad)."
      ]
    },
    {
      "metadata": {
        "id": "ZELEOVkNcuNR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "38dece15-c46f-4431-90a1-206a58fae718"
      },
      "cell_type": "code",
      "source": [
        "# mean_grads in assume to be a 1D array or list of average gradients norm per timestep memory \n",
        "np_grads = np.array([np.array(xi) for xi in grads])\n",
        "mean_grads = np_grads.mean(0)\n",
        "\n",
        "plt.bar(x=np.arange(len(mean_grads)), height=mean_grads)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 28 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jUCIX7secuNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zadanie 3\n",
        "Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n",
        "\n",
        "![lstm](utils/lstm.png)\n",
        "\n",
        "* W klasie `LSTMCell` ma znaleźć się główna loginka LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n",
        "* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przeniesiemy to do klasy modelu.\n",
        "* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n",
        "\n",
        "Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n",
        "\n",
        "Zadaniem jest osiągnięcie dokładności na poziomie przynajmniej 90%, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmienia `random seed`.\n",
        "\n",
        "### Komórka LSTM"
      ]
    },
    {
      "metadata": {
        "id": "3LndKusMcuNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # initialize LSTM weights \n",
        "        # NOTE: there are different approaches that are all correct \n",
        "        # (e.g. single matrix for all input opperations), you can pick\n",
        "        # whichever you like for this task\n",
        "        \n",
        "        self.input2hidden = torch.nn.Linear(input_size, hidden_size) \n",
        "        self.forget_gate = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.input_gate = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.memory_init = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.move_to_output = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \n",
        "        hidden, cell = states\n",
        "        \n",
        "        # Compute input, forget, and output gates\n",
        "        # then compute new cell state and hidden state\n",
        "        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
        "        combined = self.input2hidden(input) + hidden\n",
        "        \n",
        "        f_t = torch.sigmoid(self.forget_gate(combined))\n",
        "        cell_prim = torch.tanh(self.memory_init(combined))\n",
        "        i_t = torch.sigmoid(self.input_gate(combined))\n",
        "        cell = f_t * cell + i_t * cell_prim\n",
        "        \n",
        "        o_t = torch.sigmoid(self.move_to_output(combined))\n",
        "        hidden = o_t * torch.tanh(cell)\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNDTL-08cuNY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Klasa modelu LSTM"
      ]
    },
    {
      "metadata": {
        "id": "6CSICz-NcuNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = 10\n",
        "\n",
        "        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, \n",
        "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = input.shape[0]\n",
        "        \n",
        "        initial_states = self.init_hidden_cell(batch_size)\n",
        "        \n",
        "        hiddens = []\n",
        "        cells = []\n",
        "        \n",
        "        hidden, cell = initial_states\n",
        "\n",
        "        # this time we will process the whole sequence in the forward method\n",
        "        # as oppose to the previous exercise, remember to loop over the timesteps\n",
        "        \n",
        "        seq_len = input.shape[1]\n",
        "        \n",
        "        for t in range(seq_len):\n",
        "            hidden, cell = self.cell(input[:,t,:], (hidden, cell))\n",
        "            hiddens.append(hidden)\n",
        "            cells.append(cell)\n",
        "        \n",
        "        hiddens = torch.stack(hiddens)\n",
        "        \n",
        "        cells = torch.stack(cells)\n",
        "        \n",
        "        return hiddens, cells\n",
        "    \n",
        "    def init_hidden_cell(self, batch_size):\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden and cell states\n",
        "        \"\"\"\n",
        "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True), \n",
        "                torch.zeros(batch_size, self.hidden_size, requires_grad=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evG6WcG8cuNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "metadata": {
        "id": "hOav2TA5cuNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "294ea4d3-3626-40f5-ae59-987b4f345533"
      },
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# build data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# initialize the lstm with an additional cliassifier layer at the top\n",
        "lstm = LSTM(input_size=28, hidden_size=64)\n",
        "clf = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "# initialize a optimizer\n",
        "params = chain(lstm.parameters(), clf.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.01) \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epoch = 1\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epoch):\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the sequence length here\n",
        "        hiddens, cells = lstm(x)\n",
        "        outputs = clf(hiddens)\n",
        "        output = outputs[-1]\n",
        "        # calucate the loss\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()                                \n",
        "        \n",
        "        if i % 50 == 1:\n",
        "            print(f\"Epoch: {epoch} Iter: {i}/{len(train_loader)} Loss: {loss:.4f}\")\n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    \n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        \n",
        "        hiddens, cells = lstm(x)\n",
        "        outputs = clf(hiddens)\n",
        "        output = outputs[-1]\n",
        "        \n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += int(sum(pred == y))\n",
        "    \n",
        "    accuracy = correct / (batch_size * len(test_loader))\n",
        "    \n",
        "    print(f\"Final Accuracy: {accuracy}\")\n",
        "    assert accuracy > 0.9, \"Subject to random seed you should get over 0.9 accuracy, try changing the seed!\""
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Iter: 1/1200 Loss: 2.3013\n",
            "Epoch: 0 Iter: 51/1200 Loss: 1.1511\n",
            "Epoch: 0 Iter: 101/1200 Loss: 0.7476\n",
            "Epoch: 0 Iter: 151/1200 Loss: 0.7537\n",
            "Epoch: 0 Iter: 201/1200 Loss: 0.8660\n",
            "Epoch: 0 Iter: 251/1200 Loss: 0.5561\n",
            "Epoch: 0 Iter: 301/1200 Loss: 0.3881\n",
            "Epoch: 0 Iter: 351/1200 Loss: 0.7335\n",
            "Epoch: 0 Iter: 401/1200 Loss: 0.3890\n",
            "Epoch: 0 Iter: 451/1200 Loss: 0.4592\n",
            "Epoch: 0 Iter: 501/1200 Loss: 0.5516\n",
            "Epoch: 0 Iter: 551/1200 Loss: 0.1947\n",
            "Epoch: 0 Iter: 601/1200 Loss: 0.3314\n",
            "Epoch: 0 Iter: 651/1200 Loss: 0.4704\n",
            "Epoch: 0 Iter: 701/1200 Loss: 0.1694\n",
            "Epoch: 0 Iter: 751/1200 Loss: 0.2559\n",
            "Epoch: 0 Iter: 801/1200 Loss: 0.2559\n",
            "Epoch: 0 Iter: 851/1200 Loss: 0.1106\n",
            "Epoch: 0 Iter: 901/1200 Loss: 0.3667\n",
            "Epoch: 0 Iter: 951/1200 Loss: 0.2223\n",
            "Epoch: 0 Iter: 1001/1200 Loss: 0.2610\n",
            "Epoch: 0 Iter: 1051/1200 Loss: 0.1287\n",
            "Epoch: 0 Iter: 1101/1200 Loss: 0.3183\n",
            "Epoch: 0 Iter: 1151/1200 Loss: 0.1378\n",
            "Final Accuracy: 0.9376\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}